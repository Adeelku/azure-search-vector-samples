{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search Backup and Restore Sample\n",
    "\n",
    "This notebook demonstrates how to backup and restore a search index and migrate it to another instance.\n",
    "\n",
    "The only pre-requsitite is that your search index has a `key` field that is `filterable` and `sortable`. If you don't have one, you can create a new field and assign unique values to your search index. \n",
    "\n",
    "It is important to note that only fields marked as `retrievable` can be successfully backed up and restored. It's crucial to consider whether or not you want your vector fields to be marked as `retrievable` in your Cognitive Search index. Marking vector fields as `retrievable` will allow you to backup and restore them and use them for any purpose, whereas NOT marking them as `retrievable` will save you storage costs, but the tradeoff is that you will not be able to backup and restore those fields.\n",
    "\n",
    "Please review this sample and follow the instructions provided in this Jupyter Python notebook to backup and restore your Azure Cognitive Search indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-search-documents --pre\n",
    "! pip install tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script demonstrates backing up and restoring an Azure Cognitive Search index between two services. The `backup_and_restore_index` function retrieves the source index definition, creates a new target index, backs up all documents, and restores them to the target index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backing up documents:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2681468/2681468 [00:00<00:00, 3988426.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring documents:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2681468/2681468 [24:00<00:00, 1861.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents uploaded successfully.\n",
      "Successfully backed up 'beir-nq-2022-09-29' and restored to 'beir-nq'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.indexes.models import SearchIndex, SearchField, VectorSearch  \n",
    "import tqdm  \n",
    "  \n",
    "def create_clients(endpoint, key, index_name):  \n",
    "    search_client = SearchClient(endpoint=endpoint, index_name=index_name, credential=AzureKeyCredential(key))  \n",
    "    index_client = SearchIndexClient(endpoint=endpoint, credential=AzureKeyCredential(key))  \n",
    "    return search_client, index_client  \n",
    "  \n",
    "def search_results(search_client, key_field_name, top=1000):  \n",
    "    last_key = None  \n",
    "    while True:  \n",
    "        query_kwargs = {  \n",
    "            \"search_text\": \"*\",  \n",
    "            \"top\": top,  \n",
    "            \"order_by\": key_field_name  \n",
    "        }  \n",
    "        if last_key:  \n",
    "            query_kwargs[\"filter\"] = f\"{key_field_name} gt '{last_key}'\"  \n",
    "          \n",
    "        response = search_client.search(**query_kwargs)  \n",
    "        results_count = 0  \n",
    "        for result in response:  \n",
    "            yield result  \n",
    "            results_count += 1  \n",
    "            last_key = result[key_field_name]  \n",
    "          \n",
    "        if results_count < top:  \n",
    "            break  \n",
    "\n",
    "def backup_and_restore_index(source_endpoint, source_key, source_index_name, target_endpoint, target_key, target_index_name, key_field_name):  \n",
    "    # Create search and index clients  \n",
    "    source_search_client, source_index_client = create_clients(source_endpoint, source_key, source_index_name)  \n",
    "    target_search_client, target_index_client = create_clients(target_endpoint, target_key, target_index_name)  \n",
    "  \n",
    "    # Get the source index definition  \n",
    "    source_index = source_index_client.get_index(name=source_index_name)  \n",
    "  \n",
    "    # Create target index with the same definition  \n",
    "    target_index = SearchIndex(  \n",
    "        name=target_index_name,  \n",
    "        fields=[SearchField(**field.as_dict()) for field in source_index.fields],  \n",
    "        vector_search=VectorSearch(**source_index.vector_search.as_dict()) if source_index.vector_search else None  \n",
    "    )  \n",
    "    target_index_client.create_index(target_index)  \n",
    "  \n",
    "    # Backup and restore documents  \n",
    "    all_documents = list(search_results(source_search_client, key_field_name))  \n",
    "  \n",
    "    print(\"Backing up documents:\")  \n",
    "    with tqdm.tqdm(total=len(all_documents)) as progress_bar:  \n",
    "        for result in all_documents:  \n",
    "            progress_bar.update(1)  \n",
    "  \n",
    "    print(\"Restoring documents:\")  \n",
    "    failed_documents = 0  \n",
    "    failed_keys = []  \n",
    "    with tqdm.tqdm(total=len(all_documents)) as progress_bar:  \n",
    "        for batch_start in range(0, len(all_documents), 1000):  \n",
    "            batch_end = min(batch_start + 1000, len(all_documents))  \n",
    "            batch = all_documents[batch_start:batch_end]  \n",
    "            result = target_search_client.upload_documents(documents=batch)  \n",
    "            progress_bar.update(len(result))  \n",
    "  \n",
    "            for item in result:  \n",
    "                if item.succeeded is not True:  \n",
    "                    failed_documents += 1  \n",
    "                    failed_keys.append(batch[result.index_of(item)].id)  \n",
    "                    print(f\"Document upload error: {item.error.message}\")  \n",
    "  \n",
    "    if failed_documents > 0:  \n",
    "        print(f\"Failed documents: {failed_documents}\")  \n",
    "        print(f\"Failed document keys: {failed_keys}\")  \n",
    "    else:  \n",
    "        print(\"All documents uploaded successfully.\")  \n",
    "  \n",
    "    print(f\"Successfully backed up '{source_index_name}' and restored to '{target_index_name}'\")  \n",
    "    return source_search_client, target_search_client, all_documents  \n",
    "  \n",
    "# Replace with your service endpoints, keys, and index names  \n",
    "source_endpoint = \"YOUR_SEARCH_SERVICE_SOURCE_ENDPOINT\"  \n",
    "source_key = \"YOUR_SEARCH_SERVICE_SOURCE_ADMIN_KEY\"  \n",
    "source_index_name = \"YOUR_SEARCH_SERVICE_SOURCE_INDEX_NAME\"  \n",
    "target_endpoint = \"YOUR_SEARCH_SERVICE_TARGET_ENDPOINT\" \n",
    "target_key = \"YOUR_SEARCH_SERVICE_TARGET_ADMIN_KEY\"  \n",
    "target_index_name = \"YOUR_SEARCH_SERVICE_TARGET_INDEX_NAME\"  \n",
    "# Replace with the name of the key (e.g. ID, AzureDocumentKey) field in your index, this should be a unique field\n",
    "key_field_name = \"YOUR_KEY_FIELD_NAME\"  \n",
    "  \n",
    "source_search_client, target_search_client, all_documents = backup_and_restore_index(source_endpoint, source_key, source_index_name, target_endpoint, target_key, target_index_name, key_field_name)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verify_counts function compares document counts between source and target indexes after backup and restore. It prints a message indicating if the document counts match or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source document count: 2681468\n",
      "Target document count: 2681468\n",
      "Document counts match.\n"
     ]
    }
   ],
   "source": [
    "def verify_counts(source_search_client, target_search_client):  \n",
    "    source_document_count = source_search_client.get_document_count()  \n",
    "    target_document_count = target_search_client.get_document_count()  \n",
    "  \n",
    "    print(f\"Source document count: {source_document_count}\")  \n",
    "    print(f\"Target document count: {target_document_count}\")  \n",
    "  \n",
    "    if source_document_count == target_document_count:  \n",
    "        print(\"Document counts match.\")  \n",
    "    else:  \n",
    "        print(\"Document counts do not match.\")  \n",
    "  \n",
    "# Call the verify_counts function with the search_clients returned by the backup_and_restore_index function  \n",
    "verify_counts(source_search_client, target_search_client)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
